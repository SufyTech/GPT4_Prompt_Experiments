# GPT-4 Prompt Experiments

This repository contains my self-initiated hands-on experiments with GPT-4 prompts, aimed at learning prompt engineering, understanding LLM behavior, and evaluating output quality. The focus of these experiments is on iterative prompt design, tone/style variations, and clarity testing.

## Key Learnings
- Iterative prompt design improves response relevance and accuracy.
- Few-shot and zero-shot prompting helps guide model behavior.
- Clear and structured prompts generate more precise outputs.

## Experiments
1. **Basic Prompt Variation** – Testing simple, casual, and persuasive prompts.
2. **Tone & Style Variation** – Formal, humorous, and inspirational tone experiments.
3. **Prompt Clarity Test** – Comparing vague, clear, and detailed prompts for output quality.

**Note:** Outputs are simulated for demonstration purposes due to GPT-4 subscription constraints.

**Screenshots** and **notebooks** demonstrate the outputs along with my observations.

## Next Steps
- Experiment with context-based prompts and retrieval-augmented generation (RAG).
- Explore AI applications for socially impactful solutions.

**Skills demonstrated:** Python, prompt engineering, LLM experimentation, iterative analysis, and documentation.
